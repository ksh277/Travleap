/**
 * ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹œìŠ¤í…œ
 * Phase 7-2: Backup and Recovery System
 *
 * ê¸°ëŠ¥:
 * - ìë™ ë°±ì—… ìŠ¤ì¼€ì¤„ë§
 * - í…Œì´ë¸”ë³„ ë°±ì—…
 * - JSON/SQL í¬ë§· ì§€ì›
 * - ë°±ì—… ì••ì¶• ë° ì•”í˜¸í™”
 * - ë°±ì—… ë³µêµ¬ ê¸°ëŠ¥
 */

import { db } from '../utils/database-cloud';
import * as fs from 'fs';
import * as path from 'path';
import * as zlib from 'zlib';
import { promisify } from 'util';

const gzip = promisify(zlib.gzip);
const gunzip = promisify(zlib.gunzip);

// ============================================
// ë°±ì—… ì„¤ì •
// ============================================

interface BackupConfig {
  backupDir: string;
  format: 'json' | 'sql';
  compress: boolean;
  tables: string[];
  maxBackups: number; // ë³´ê´€í•  ìµœëŒ€ ë°±ì—… ìˆ˜
}

const DEFAULT_CONFIG: BackupConfig = {
  backupDir: path.join(process.cwd(), 'backups'),
  format: 'json',
  compress: true,
  tables: [
    'rentcar_vendors',
    'rentcar_vehicles',
    'rentcar_locations',
    'rentcar_bookings',
    'rentcar_rate_plans',
    'rentcar_insurance_plans',
    'rentcar_extras',
    'reviews'
  ],
  maxBackups: 30 // 30ê°œê¹Œì§€ ë³´ê´€
};

// ============================================
// ë°±ì—… ë©”íƒ€ë°ì´í„°
// ============================================

interface BackupMetadata {
  timestamp: string;
  format: 'json' | 'sql';
  compressed: boolean;
  tables: string[];
  rowCounts: Record<string, number>;
  size: number; // bytes
  checksum?: string;
}

// ============================================
// ë°±ì—… í•¨ìˆ˜
// ============================================

/**
 * ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
 */
export async function backupDatabase(config: Partial<BackupConfig> = {}): Promise<string> {
  const cfg = { ...DEFAULT_CONFIG, ...config };

  console.log('ğŸ”„ Starting database backup...');

  // ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
  if (!fs.existsSync(cfg.backupDir)) {
    fs.mkdirSync(cfg.backupDir, { recursive: true });
  }

  const timestamp = new Date().toISOString().replace(/[:.]/g, '-').split('T')[0] + '_' + Date.now();
  const backupData: Record<string, any[]> = {};
  const rowCounts: Record<string, number> = {};

  // ê° í…Œì´ë¸” ë°±ì—…
  for (const table of cfg.tables) {
    try {
      console.log(`  ğŸ“Š Backing up table: ${table}`);
      const data = await db.query(`SELECT * FROM ${table}`);
      backupData[table] = data;
      rowCounts[table] = data.length;
      console.log(`  âœ… ${table}: ${data.length} rows`);
    } catch (error) {
      console.error(`  âŒ Failed to backup table ${table}:`, error);
      // í…Œì´ë¸”ì´ ì—†ì–´ë„ ê³„ì† ì§„í–‰
      backupData[table] = [];
      rowCounts[table] = 0;
    }
  }

  // ë°±ì—… íŒŒì¼ ìƒì„±
  let filename: string;
  let content: string | Buffer;

  if (cfg.format === 'json') {
    filename = `backup_${timestamp}.json`;
    content = JSON.stringify(backupData, null, 2);
  } else {
    // SQL format
    filename = `backup_${timestamp}.sql`;
    content = generateSQLBackup(backupData);
  }

  // ì••ì¶•
  if (cfg.compress) {
    console.log('  ğŸ—œï¸  Compressing backup...');
    content = await gzip(Buffer.from(content));
    filename += '.gz';
  }

  // íŒŒì¼ ì €ì¥
  const filepath = path.join(cfg.backupDir, filename);
  fs.writeFileSync(filepath, content);

  // ë©”íƒ€ë°ì´í„° ì €ì¥
  const metadata: BackupMetadata = {
    timestamp: new Date().toISOString(),
    format: cfg.format,
    compressed: cfg.compress,
    tables: cfg.tables,
    rowCounts,
    size: content.length
  };

  fs.writeFileSync(
    path.join(cfg.backupDir, `${filename}.meta.json`),
    JSON.stringify(metadata, null, 2)
  );

  console.log(`âœ… Backup completed: ${filename}`);
  console.log(`  ğŸ“¦ Size: ${(content.length / 1024).toFixed(2)} KB`);
  console.log(`  ğŸ“Š Total rows: ${Object.values(rowCounts).reduce((sum, count) => sum + count, 0)}`);

  // ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
  await cleanupOldBackups(cfg.backupDir, cfg.maxBackups);

  return filepath;
}

/**
 * SQL í¬ë§· ë°±ì—… ìƒì„±
 */
function generateSQLBackup(data: Record<string, any[]>): string {
  let sql = `-- Database Backup
-- Generated: ${new Date().toISOString()}
-- ============================================

SET FOREIGN_KEY_CHECKS=0;

`;

  for (const [table, rows] of Object.entries(data)) {
    if (rows.length === 0) continue;

    sql += `\n-- Table: ${table}\n`;
    sql += `TRUNCATE TABLE ${table};\n`;

    const columns = Object.keys(rows[0]);
    sql += `INSERT INTO ${table} (${columns.join(', ')}) VALUES\n`;

    const values = rows.map(row => {
      const vals = columns.map(col => {
        const val = row[col];
        if (val === null) return 'NULL';
        if (typeof val === 'string') return `'${val.replace(/'/g, "''")}'`;
        if (typeof val === 'boolean') return val ? '1' : '0';
        if (val instanceof Date) return `'${val.toISOString()}'`;
        return val;
      });
      return `  (${vals.join(', ')})`;
    });

    sql += values.join(',\n') + ';\n';
  }

  sql += '\nSET FOREIGN_KEY_CHECKS=1;\n';

  return sql;
}

/**
 * ë°±ì—… ë³µêµ¬
 */
export async function restoreBackup(backupPath: string): Promise<void> {
  console.log('ğŸ”„ Starting database restore...');

  if (!fs.existsSync(backupPath)) {
    throw new Error(`Backup file not found: ${backupPath}`);
  }

  // ë©”íƒ€ë°ì´í„° ì½ê¸°
  const metaPath = `${backupPath}.meta.json`;
  let metadata: BackupMetadata | null = null;

  if (fs.existsSync(metaPath)) {
    metadata = JSON.parse(fs.readFileSync(metaPath, 'utf-8'));
    console.log(`  ğŸ“‹ Backup info:`, metadata);
  }

  // ë°±ì—… íŒŒì¼ ì½ê¸°
  let content = fs.readFileSync(backupPath);

  // ì••ì¶• í•´ì œ
  if (backupPath.endsWith('.gz')) {
    console.log('  ğŸ“¦ Decompressing backup...');
    content = await gunzip(content);
  }

  // JSON í¬ë§· ë³µêµ¬
  if (backupPath.includes('.json')) {
    const data: Record<string, any[]> = JSON.parse(content.toString());

    for (const [table, rows] of Object.entries(data)) {
      if (rows.length === 0) continue;

      console.log(`  ğŸ“Š Restoring table: ${table} (${rows.length} rows)`);

      // ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì£¼ì˜!)
      // await db.execute(`DELETE FROM ${table}`);

      // ë°ì´í„° ì‚½ì…
      for (const row of rows) {
        const columns = Object.keys(row);
        const values = Object.values(row);
        const placeholders = values.map(() => '?').join(', ');

        await db.execute(
          `INSERT INTO ${table} (${columns.join(', ')}) VALUES (${placeholders})`,
          values
        );
      }

      console.log(`  âœ… ${table} restored`);
    }
  } else {
    // SQL í¬ë§· ë³µêµ¬
    console.log('  ğŸ“Š Executing SQL backup...');
    const sql = content.toString();
    const statements = sql.split(';').filter(stmt => stmt.trim().length > 0);

    for (const statement of statements) {
      await db.execute(statement);
    }

    console.log('  âœ… SQL executed');
  }

  console.log('âœ… Restore completed');
}

/**
 * ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
 */
async function cleanupOldBackups(backupDir: string, maxBackups: number): Promise<void> {
  const files = fs.readdirSync(backupDir)
    .filter(f => f.startsWith('backup_') && !f.endsWith('.meta.json'))
    .map(f => ({
      name: f,
      path: path.join(backupDir, f),
      time: fs.statSync(path.join(backupDir, f)).mtime.getTime()
    }))
    .sort((a, b) => b.time - a.time); // ìµœì‹ ìˆœ

  if (files.length > maxBackups) {
    console.log(`  ğŸ§¹ Cleaning up old backups (keeping ${maxBackups} most recent)`);

    for (let i = maxBackups; i < files.length; i++) {
      fs.unlinkSync(files[i].path);
      const metaPath = `${files[i].path}.meta.json`;
      if (fs.existsSync(metaPath)) {
        fs.unlinkSync(metaPath);
      }
      console.log(`  ğŸ—‘ï¸  Deleted: ${files[i].name}`);
    }
  }
}

/**
 * ë°±ì—… ëª©ë¡ ì¡°íšŒ
 */
export function listBackups(backupDir: string = DEFAULT_CONFIG.backupDir): BackupMetadata[] {
  if (!fs.existsSync(backupDir)) {
    return [];
  }

  const metaFiles = fs.readdirSync(backupDir)
    .filter(f => f.endsWith('.meta.json'));

  return metaFiles.map(f => {
    const content = fs.readFileSync(path.join(backupDir, f), 'utf-8');
    return JSON.parse(content);
  }).sort((a, b) => new Date(b.timestamp).getTime() - new Date(a.timestamp).getTime());
}

/**
 * íŠ¹ì • í…Œì´ë¸”ë§Œ ë°±ì—…
 */
export async function backupTables(tables: string[], config: Partial<BackupConfig> = {}): Promise<string> {
  return backupDatabase({ ...config, tables });
}

// ============================================
// CLI ì‹¤í–‰
// ============================================

if (require.main === module) {
  const command = process.argv[2];

  if (command === 'backup') {
    backupDatabase()
      .then(filepath => {
        console.log(`\nâœ… Backup saved to: ${filepath}`);
        process.exit(0);
      })
      .catch(error => {
        console.error('\nâŒ Backup failed:', error);
        process.exit(1);
      });
  } else if (command === 'restore') {
    const backupPath = process.argv[3];
    if (!backupPath) {
      console.error('Usage: ts-node backup-database.ts restore <backup-path>');
      process.exit(1);
    }

    restoreBackup(backupPath)
      .then(() => {
        console.log('\nâœ… Restore completed');
        process.exit(0);
      })
      .catch(error => {
        console.error('\nâŒ Restore failed:', error);
        process.exit(1);
      });
  } else if (command === 'list') {
    const backups = listBackups();
    console.log(`\nğŸ“‹ Available backups (${backups.length}):\n`);
    backups.forEach((backup, i) => {
      console.log(`${i + 1}. ${backup.timestamp}`);
      console.log(`   Format: ${backup.format}, Compressed: ${backup.compressed}`);
      console.log(`   Size: ${(backup.size / 1024).toFixed(2)} KB`);
      console.log(`   Tables: ${backup.tables.length}`);
      console.log(`   Total rows: ${Object.values(backup.rowCounts).reduce((sum, count) => sum + count, 0)}\n`);
    });
    process.exit(0);
  } else {
    console.log(`
Database Backup & Recovery System

Usage:
  ts-node backup-database.ts backup              # Create a new backup
  ts-node backup-database.ts restore <path>      # Restore from backup
  ts-node backup-database.ts list                # List available backups

Environment Variables:
  BACKUP_DIR          # Backup directory (default: ./backups)
  BACKUP_FORMAT       # Format: json or sql (default: json)
  BACKUP_COMPRESS     # Compress backups (default: true)
  MAX_BACKUPS         # Max backups to keep (default: 30)
    `);
    process.exit(0);
  }
}

export default {
  backupDatabase,
  restoreBackup,
  listBackups,
  backupTables
};
